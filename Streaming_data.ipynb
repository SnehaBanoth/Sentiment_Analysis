{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/samcoopmans/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/samcoopmans/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/samcoopmans/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import KafkaClient, SimpleProducer\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, StringType, LongType, IntegerType, BinaryType, TimestampType, ArrayType\n",
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import io\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp\n",
    "from pyspark.sql.functions import array, lit, struct\n",
    "\n",
    "import sys\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "import tweepy \n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "from sparkdl import KerasTransformer\n",
    "from pyspark.sql.functions import split, array_remove, size, to_json\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, ArrayType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model\n",
    "## Training the model using batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"train_data.csv\")\n",
    "\n",
    "train_data = train_data.selectExpr(\"sentiment as label\", \"tweet as text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete NULL Values from training data\n",
    "cleaned_text_train = train_data.na.drop(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenized_train_df = regexTokenizer.transform(cleaned_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=100000, minDF=1.0)\n",
    "vec_model = cv.fit(tokenized_train_df)\n",
    "count_df_train = vec_model.transform(tokenized_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(count_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"test_data.csv\")\n",
    "\n",
    "test_data = test_data.selectExpr(\"sentiment as label\", \"tweet as text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete NULL Values from testing data\n",
    "cleaned_text_test = test_data.na.drop(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenized_test_df = regexTokenizer.transform(cleaned_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "count_df_test = vec_model.transform(tokenized_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "predictions = model.transform(count_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.7671323369813113\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start predicting STREAM of tweets (Spark Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampFormat = \"E MMM dd HH:mm:ss +0000 yyyy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TwitterStreaming\").getOrCreate()\n",
    "\n",
    "df = spark.readStream \\\n",
    "          .format(\"kafka\") \\\n",
    "          .option(\"kafka.bootstrap.servers\", \"34.90.212.110:9092\") \\\n",
    "          .option(\"startingOffsets\", \"earliest\") \\\n",
    "          .option(\"subscribe\", \"weather\") \\\n",
    "          .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_value_df = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_schema = StructType([StructField(\"timestamp\", StringType(), False),\n",
    "                     StructField(\"id\", StringType(), False),\n",
    "                     StructField(\"tweet\", StringType(), False),\n",
    "                     StructField(\"city\", StringType(), False),\n",
    "                     StructField(\"temperature\", StringType(), False),\n",
    "                     StructField(\"description\", StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_value_df = raw_value_df.select(from_json(\"value\", twitter_schema).alias('weather'))\\\n",
    "                            .select('weather.*')\n",
    "\n",
    "json_value_df = json_value_df.withColumn('timestamp',to_timestamp(json_value_df.timestamp, timestampFormat))\n",
    "json_value_df = json_value_df.withColumn(\"temperature\", json_value_df[\"temperature\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the stream of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_udf(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet_split = tweet.split()\n",
    "    \n",
    "    for i in range(len(tweet_split)):\n",
    "        if tweet_split[i].startswith('@'):\n",
    "            tweet_split[i] = 'mention'\n",
    "        elif tweet_split[i].startswith('http://'):\n",
    "            tweet_split[i] = 'link'\n",
    "        elif tweet_split[i].startswith('https://'):\n",
    "            tweet_split[i] = 'link'\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    tweet_without_mention = \" \".join(tweet_split)\n",
    "    \n",
    "    tweet_without_mention = re.sub('<.*?>', '', tweet_without_mention) # remove HTML tags\n",
    "    tweet_without_mention = re.sub(r'[^\\w\\s]', ' ', tweet_without_mention) # remove punc\n",
    "    tweet_without_mention = re.sub(r'\\d+', '', tweet_without_mention) # remove numbers\n",
    "    tweet_without_mention = re.sub('  ', ' ', tweet_without_mention)\n",
    "    \n",
    "    spelling_correct = eval(open('mistakes.txt', 'r').read())\n",
    "    \n",
    "    tweet_split_2 = word_tokenize(tweet_without_mention)\n",
    "    replaced = [spelling_correct[word] if word in spelling_correct else word for word in tweet_split_2]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in replaced]\n",
    "    \n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    stop_words.extend(['link', 'mention'])\n",
    "    \n",
    "    filtered_tweet = [word for word in lemmatized if not word in stop_words]\n",
    "    no_short_words = [word for word in filtered_tweet if len(word)>2]\n",
    "    no_too_long_words = [word for word in no_short_words if len(word)<15]\n",
    "\n",
    "    full_tweet = \" \".join(no_too_long_words)\n",
    "    \n",
    "    return full_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = spark.udf.register(\"preprocessed\", preprocessed_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_df = json_value_df.select(col(\"timestamp\"), col(\"id\"), col('tweet'), col('city'), col('temperature'), col('description'), preprocessed(col(\"tweet\")).alias(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Null values\n",
    "clean_tweets_1 = clean_tweets_df.na.drop(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "clean_tweets_tokenized = regexTokenizer.transform(clean_tweets_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization\n",
    "clean_tweets_vectorized = vec_model.transform(clean_tweets_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict tweets based on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the tweets based on Trained Model\n",
    "tweets_predictions = model.transform(clean_tweets_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df = tweets_predictions.select(col('id'), col('timestamp'), col('tweet'), col('text'), col('temperature'), col('city'), col('description'), col('prediction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push STREAM DATAFRAME to Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import KafkaClient, SimpleProducer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, array, to_json, col, struct\n",
    "final_df_json = final_df.select(to_json(struct([col(c).alias(c) for c in final_df.columns])).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = final_df_json \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"34.90.212.110:9092\") \\\n",
    "  .option(\"topic\", \"predictions_7\") \\\n",
    "  .option(\"checkpointLocation\", \"predictions_7\")\\\n",
    "  .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
